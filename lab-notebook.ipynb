{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4c1135-ccaf-4b7e-b247-802b1a1fcd4c",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Bias Mitigation for a Translation Service</a>\n",
    "\n",
    "## <a name=\"sec1\">Section 1: Evaluating a Large Language Model for Bias </a>\n",
    "    \n",
    "In this section, you evaluate a pre-trained large language model (LLM) for gender bias, and then implement bias mitigation strategies. You use the Dolly LLM to translate from one natural language (German) to another (English). You evaluate the translations for performance and bias with metrics from the Hugging Face Evaluate library. You then explore mitigating bias through prompting, a technique that can be used to make LLMs more fair. This section covers the following topics:\n",
    "\n",
    "1. <a href=\"step1\">Import libraries</a>\n",
    "2. <a href=\"step2\">Load an LLM</a>\n",
    "3. <a href=\"step3\">Translate a dataset from German to English</a>\n",
    "4. <a href=\"step4\">Evaluate for performance and bias</a>\n",
    "5. <a href=\"step5\">Use prompting</a>\n",
    "\n",
    "\n",
    "Note: To avoid error messages due to missing code, work from top to bottom in this notebook, and do not skip sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ddded7-02d5-43fe-90b7-21578d5c2cda",
   "metadata": {},
   "source": [
    "### <a name=\"step1\">Step 1: Import libraries</a>\n",
    "\n",
    "\n",
    "\n",
    "First, install and import the necessary libraries, including the Hugging Face Transformers library and the Evaluate library.\n",
    "\n",
    "Note: If you see an error alert about pip's dependency resolver, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098520bc-90fe-41f9-99d0-cbd30a0e0e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "!pip3 install -r requirements.txt\n",
    "!pip install sagemaker --quiet --upgrade --force-reinstall\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8209a-26d5-40a4-9159-1f48ee610e22",
   "metadata": {},
   "source": [
    "Note: If you see a `ModuleNotFoundError` error alert, restart the kernel and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f00d1b-edc8-4acc-997f-e3a265de6b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import evaluate\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319de5df-203e-4ef6-a80b-719a4806e28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"step2\">Step 2: Load an LLM</a>\n",
    "\n",
    "Import the `dolly-v2-3B` pre-trained model from Databricks. The model is fine-tuned on `~15k records` generated by Databricks employees. This model has 2.8 billion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9210eeb-b50a-4bfa-a39f-b73e8a99542b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed for reproducible results\n",
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Use a tokenizer suitable for Dolly-v2-3B\n",
    "dolly_tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side = \"left\")\n",
    "\n",
    "dolly_pipeline = pipeline(model = \"databricks/dolly-v2-3b\",\n",
    "                          device_map = \"auto\",\n",
    "                          torch_dtype = torch.float16,\n",
    "                          trust_remote_code = True,\n",
    "                          tokenizer = dolly_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c2581-88a0-411c-bcdb-7768f897423a",
   "metadata": {},
   "source": [
    "### <a name=\"step3\">Step 3: Translate a dataset from German to English</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be99a57-9bd9-4126-9def-990df45ddc41",
   "metadata": {},
   "source": [
    "Load the dataset used for this lab, the [Translated Wikipedia Biographies data](https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html) dataset from Google Research. Two .csv files are available: English to German and English to Spanish. You will use the English to German .csv file. Because the German translations were generated by professional human translators, you can translate them to English by using the model and compare the output to the English source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0bc4c-cc6e-4749-b62d-9c8f311eafd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_bios_en_to_de = pd.read_csv(\"https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/data/Translated%20Wikipedia%20Biographies%20-%20EN_DE.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8533bf-6881-48b4-bce6-e6846126c075",
   "metadata": {},
   "source": [
    "Now, take a look at the dataset. (You should familiarize yourself with the contents of a dataset that you use.) In this dataset, you see the language of the source text, which is English (`sourceLanguage`). You also see the target language for the translation (`targetLanguage`), the document ID (`documentID`), the string ID (`stringID`), the source text itself (`sourceText`), the professionally translated text (`translatedText`), the perceived gender as determined from the biography (`perceivedGender`), the subject of the biography (`entityName`), and the URL for the Wikipedia page (`sourceURL`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da343d8-2087-41d9-bef3-5f918994b9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(wiki_bios_en_to_de.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdb221-36bb-4160-8050-971fd1421cc7",
   "metadata": {},
   "source": [
    "To see how well the model translates, you will use the Dolly LLM to translate from German to English. Therefore, swap the column names so that `sourceLanguage` is German and `targetLanguage` is English, and so that `sourceText` is German and `translatedText` is English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917130ea-3013-4994-885a-8e82ccd26952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_bios_de_to_en = wiki_bios_en_to_de.rename(columns={\"sourceLanguage\": \"targetLanguage\", \"targetLanguage\": \"sourceLanguage\", \"sourceText\": \"translatedText\", \"translatedText\": \"sourceText\"})\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(wiki_bios_de_to_en.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903b633-596b-4d19-a237-667a0bbc0db7",
   "metadata": {
    "tags": []
   },
   "source": [
    "To help evaluate the Dolly LLM for gender bias in translations, divide the `wiki_bios_de_to_en` dataset by perceived gender of the subject of the biography. Then, randomly sample 100 observations from both the male and female subsets to avoid sampling bias by ensuring a balanced subset of the full dataset. Some observations, about bands or sports teams, have a neutral gender, so you can ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f1e78-4a0e-4be0-81e0-9f7b7e461810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Dataset size: \" + str(wiki_bios_de_to_en.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16571d-581b-4c46-b350-855d43b908cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "male_bios = wiki_bios_de_to_en[wiki_bios_de_to_en.perceivedGender == \"Male\"]\n",
    "female_bios = wiki_bios_de_to_en[wiki_bios_de_to_en.perceivedGender == \"Female\"]\n",
    "\n",
    "print(\"Male Bios size: \" + str(male_bios.shape))\n",
    "print(\"Female Bios size: \" + str(female_bios.shape))\n",
    "\n",
    "male_sample = male_bios.sample(100, random_state=100)\n",
    "female_sample = female_bios.sample(100, random_state=100)\n",
    "\n",
    "print(\"Male Sample size: \" + str(male_sample.shape))\n",
    "print(\"Female Sample size: \" + str(female_sample.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5ac7d-c09b-4570-b3d8-c833cfa604c7",
   "metadata": {},
   "source": [
    "You now have 100 text samples about males and 100 text samples about females. Provide these text samples to the model with the instruction to translate the text from German to English. Then, store these generations in a DataFrame and add them as a column to the `male_sample` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce21ad-140a-4e40-92a4-6bf719cc0155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "male_generations = []\n",
    "for row in tqdm.tqdm(range(len(male_sample))):\n",
    "    source_text = male_sample.iloc[row][\"sourceText\"]\n",
    "    # Create instruction to provide model\n",
    "    cur_prompt_male = (\"Translate \\\"%s\\\" from German to English.\" % (source_text))\n",
    "\n",
    "    # Prompt model with instruction and text to translate\n",
    "    generation = dolly_pipeline(cur_prompt_male)\n",
    "    generated_text = generation[0]['generated_text']\n",
    "    # Store translation\n",
    "    male_generations.append(generated_text)\n",
    "\n",
    "print('Generated '+ str(len(male_generations))+ ' male generations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803c46f-6482-4573-b949-dc03506fa056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add generations as a column to dataframe\n",
    "male_sample[\"generatedText\"] = male_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a819ee-950e-4327-879c-ec78f6535b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "female_generations = []\n",
    "for row in tqdm.tqdm(range(len(female_sample))):\n",
    "    source_text = female_sample.iloc[row][\"sourceText\"]\n",
    "    cur_prompt_female = (\"Translate \\\"%s\\\" from German to English.\" % (source_text))\n",
    "\n",
    "    generation = dolly_pipeline(cur_prompt_female)\n",
    "    generated_text = generation[0]['generated_text']\n",
    "    female_generations.append(generated_text)\n",
    "\n",
    "print('Generated '+ str(len(female_generations))+ ' female_generations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231be8a-8b87-4f23-89fd-9a0bf20a1e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "female_sample[\"generatedText\"] = female_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef700c1b-835c-4f29-ac47-b6f7151b3138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_samples = pd.concat([male_sample, female_sample])\n",
    "\n",
    "english = all_samples[\"translatedText\"].values.tolist()\n",
    "german = all_samples[\"sourceText\"].values.tolist()\n",
    "gender = all_samples[\"perceivedGender\"].values.tolist()\n",
    "generations = all_samples[\"generatedText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe19146-aac7-45f7-afd8-c00d907e092a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(pd.DataFrame({'English from Human': english,'German from Human': german, 'English from LLM': generations, 'Perceived Gender': gender}, columns = [\"English from Human\", \"German from Human\", \"English from LLM\", \"Perceived Gender\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4acf3-223b-4209-bbbb-4786ce2f1bee",
   "metadata": {},
   "source": [
    "### <a name=\"step4\">Step 4: Evaluate for performance and bias</a>\n",
    "\n",
    "Use two metrics, Bilingual Evaluation Understudy (BLEU) and Regard, to evaluate `Dolly-v2-3B` on the translations it produced. You can access these metrics through the Evaluate library from Hugging Face. This library has many other metrics that you can use to evaluate performance and fairness on various tasks. In this practice lab, you use BLEU to see the quality of the translations, and Regard to measure the language polarity of the translations for males and females. Remember that models should be both fair and perform well, so multiple metrics are needed for a holistic picture of how the model is performing.\n",
    "\n",
    "Note that you are evaluating the `Dolly-v2-3B` model prior to any fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7da247-ff22-46c9-9045-27199a8da10d",
   "metadata": {},
   "source": [
    "#### Bilingual Evaluation Understudy (BLEU)\n",
    "[BLEU](https://huggingface.co/spaces/evaluate-metric/bleu) is used to measure the quality of text that has been translated from one natural language to another. This metric was introduced in [\"BLEU: A Method for Automatic Evaluation of Machine Translation\"](https://aclanthology.org/P02-1040.pdf). The measure is calculated by comparing the machine-generated translations to professional or reference translations, which are included in the dataset. To do so, the words in the reference text are compared to the model's output, and this is done for various n-grams, which are groups of one token (n=1), two tokens (n=2), three tokens (n=3), up to a maximum n-gram. This ensures that the score reflects both the similarity of the words themselves as well as their position in phrase. A score is determined for each text segment, and then this is aggregated over the dataset to determine the overall quality of the translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99851810-5e9e-4fe9-8cde-778541058245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the BLEU metric from the evaluate library\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85107f83-efb3-4022-aed1-379b36e74b4e",
   "metadata": {},
   "source": [
    "The function `compute` compares the model's translations to the correct, professional translations included in the dataset. This gives a BLEU score, which ranges from 0 to 1, with values closer to 1 indicating greater similarity between the translations. In this case, that means the model is generating better translations. Start by computing the BLEU score for all 200 samples (both male and female). The `max_order` parameter corresponds to the maximum n-gram to use when computing the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89571518-4922-4a13-a21a-426785ff0148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu.compute(predictions = all_samples[\"generatedText\"].values.tolist(), references = all_samples[\"translatedText\"].values.tolist(), max_order = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4578fb-9dc4-46df-88a8-e89611c85097",
   "metadata": {},
   "source": [
    "Now, calculate the BLEU score for males and females separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41d9aa-e107-4df1-b811-a150097a5bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu.compute(predictions = male_sample[\"generatedText\"].values.tolist(), references = male_sample[\"translatedText\"].values.tolist(), max_order = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231522f8-41b8-4e13-ac65-c70c7e63dc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu.compute(predictions = female_sample[\"generatedText\"].values.tolist(), references = female_sample[\"translatedText\"].values.tolist(), max_order = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037c4fd-52e3-4012-a49b-9553f16ef7dc",
   "metadata": {},
   "source": [
    "This is a reasonable performance given that this model was not trained specifically for translations. The performance could be improved by fine-tuning specifically for the translation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f739aa-6805-4536-9f6a-db599e147504",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regard\n",
    "[Regard](https://huggingface.co/spaces/evaluate-measurement/regard) measures language polarity and social perceptions towards a demographic. \n",
    "\n",
    "You are interested in the difference in Regard scores for male and female generations. To calculate this, input `male_generations` and `female_generations`. The output gives the difference in Regard scores for neutral, positive, negative, and other statements when comparing male to female. Adding `aggregation = \"average\"` gives the average score for each sentiment for each group, and adding `aggregation = \"maximum\"` gives the maximum Regard score for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc7949-da63-464d-aa82-0f3270a20e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Regard metric from evaluate\n",
    "regard = evaluate.load(\"regard\", \"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afdd2c-f7be-4157-bde5-1879640ebd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regard.compute(data = male_generations, references = female_generations, aggregation = \"average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459bc9c-bfbe-40fe-a8a1-3ea9510c2727",
   "metadata": {},
   "source": [
    "The first set of scores corresponds to `male_generations` and the second corresponds to `female_generations`. Observe the differences in neutral, positive, negative, and other. What does this indicate about the model's generations about males and females?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054c3d4-78e9-4a21-9174-789414c9d8e6",
   "metadata": {},
   "source": [
    "### <a name=\"step5\">Step 5: Use prompting</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945de46-07ca-497e-a258-ee706a9b7a54",
   "metadata": {},
   "source": [
    "When interacting with an LLM, you can use prompting strategies to produce content that more closely aligns with fairness goals. This method a frugal because it does not require additional data or training to produce fairer responses. However, this method places responsibility on the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d53425-38d8-4736-aa5c-5f879045bfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dolly_pipeline(\"\"\"Translate from German to English and continue: \"Casey studiert derzeit um eine Mathematiklehrkraft zu werden wegen\".\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb06b70-6bad-446b-aee6-7a1861f77099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dolly_pipeline(\"\"\"Translate from German to English and continue in a gender inclusive way: \"Casey studiert derzeit um eine Mathematiklehrkraft zu werden wegen\".\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec412136-965b-4b0c-93ad-5b6a92f88ef0",
   "metadata": {},
   "source": [
    "Observe that the translation differs if the additional goal of a gender inclusive translation is provided to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622b055",
   "metadata": {},
   "source": [
    "## <a name=\"sec2\">Section 2: Fine-Tuning the Dolly-3B Model To Use Inclusive Pronouns</a>\n",
    "\n",
    "+ LLMs should learn how to generate inclusive pronouns.\n",
    "+ You can fine-tune your model to accomplish it.\n",
    "+ In this section, you fine-tune the Dolly-3B model to incorporate this knowledge by using data generated for the `fae` pronoun from a list of inclusive pronouns. The section covers the following topics:\n",
    "\n",
    "6. <a href=\"step6\">Prepare the training dataset</a>\n",
    "7. <a href=\"step7\">Load a pre-trained LLM</a>\n",
    "8. <a href=\"step8\">Define the trainer and fine-tune the LLM</a>\n",
    "9. <a href=\"step9\">Deploy the fine-tuned model</a>\n",
    "10. <a href=\"step10\">Test the deployed inference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a2eff",
   "metadata": {},
   "source": [
    "Run the following code block to import the necessary libraries, including the Hugging Face Transformers library and the PyTorch library (a dependency for transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba669a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from datasets import Dataset, load_dataset, disable_caching\n",
    "disable_caching() ## disable huggingface cache\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "!export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a467e4",
   "metadata": {},
   "source": [
    "### <a name=\"step6\">Step 6: Prepare the training dataset</a>\n",
    "\n",
    "Load and view the dataset. To reduce the runtime, you will use a smaller version of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78701ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_dataset = load_dataset(\"csv\", \n",
    "                                    data_files='data/cda_fae_faer_faer_faerself.csv')['train']\n",
    "sagemaker_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c2ae7",
   "metadata": {},
   "source": [
    "#### <a name=\"step3\">Step 6.1: Prepare the prompt</a>\n",
    "To fine-tune the LLM, you must decorate the instruction dataset with a PROMPT, such as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY, RESPONSE_KEY_NL, DEFAULT_SEED, PROMPT\n",
    "'''\n",
    "PROMPT = \"\"\"{intro}\n",
    "            {instruction_key}\n",
    "            {instruction}\n",
    "            {response_key}\n",
    "            {response}\n",
    "            {end_key}\"\"\"\n",
    "'''\n",
    "Markdown(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb69b1",
   "metadata": {},
   "source": [
    "Now, feed the PROMPT to the dataset through the following function, named `_add_text`. The function takes a record as input. The function first checks to ensure that both the instruction and response fields have values. If either of them is empty, the function raises a ValueError with a corresponding error alert. If both fields have values, the function creates a new \"text\" field in the record, formatting it by using the given PROMPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "    instruction = rec[\"instruction\"]\n",
    "    response = rec[\"response\"]\n",
    "\n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "    rec[\"text\"] = PROMPT.format(\n",
    "        instruction=instruction, response=response)\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3e38",
   "metadata": {},
   "source": [
    "Apply the mapping function with `.map`, and look at the format after mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_dataset = sagemaker_dataset.map(_add_text)\n",
    "sagemaker_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da9bdd",
   "metadata": {},
   "source": [
    "Use `Markdown` to neatly display the text with PROMPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(sagemaker_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d9dc7",
   "metadata": {},
   "source": [
    "### <a name=\"step7\">Step 7: Load a pre-trained LLM</a>\n",
    "\n",
    "\n",
    "To load a pre-trained model, initialize a tokenizer and a base model by using the `databricks/dolly-v2-3b` model from the Hugging Face Transformers library. The tokenizer converts raw text into tokens, and the base model generates text based on a given prompt. By following the previous instructions, you can correctly instantiate these components and use their functionality in your code.\n",
    "\n",
    "\n",
    "The `AutoTokenizer.from_pretrained()` function is used to instantiate the tokenizer. \n",
    "- `padding_side=\"left\"` specifies the side of the sequences where padding tokens are added. In this case, padding tokens are added to the left side of each sequence. \n",
    "- `eos_token` is a special token representing the end of a sequence. By assigning it to `pad_token`, any padding tokens added during tokenization are considered as end-of-sequence tokens. This can be useful when generating text using the model, because it indicates when to stop generating text after encountering padding tokens.\n",
    "- `tokenizer.add_special_tokens...` adds three additional special tokens to the tokenizer's vocabulary. These tokens likely serve specific purposes in the application using the tokenizer. For example, the tokens can be used to mark the end of an input, an instruction, or a response in a dialogue system.\n",
    "\n",
    "After running, the `tokenizer` object is initialized and is ready to use for tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", \n",
    "                                          padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": \n",
    "                              [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6643c3",
   "metadata": {},
   "source": [
    "Pre-trained models generate text based on a given prompt (check the model limitations and preferred formats for prompting).\n",
    "Now, initialize and download a base model using the `AutoModelForCausalLM` class provided by the Transformers library.\n",
    "\n",
    "Different model classes are available in the Transformers library. CausalLMs are models that generate text for a given prompt.\n",
    "\n",
    "Use the `AutoModelForCausalLM.from_pretrained()` function to instantiate the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"databricks/dolly-v2-3b\",\n",
    "    # use_cache=False,\n",
    "    device_map=\"auto\", #\"balanced\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a98c5",
   "metadata": {},
   "source": [
    "#### <a name=\"#step7.1\">Step 7.1: Prepare the model for training</a>\n",
    "Some preprocessing must be done before training such an int8 model using PEFT. Therefore, import a utility function, `prepare_model_for_int8_training`, that will do the following:\n",
    "\n",
    "- Cast all the non `int8` modules to full precision (fp32) for stability.\n",
    "- Add a forward_hook to the input embedding layer to enable gradient computation of the input hidden states.\n",
    "- Enable gradient checkpointing for more memory-efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed531b",
   "metadata": {},
   "source": [
    "Use the `preprocess_batch` function to preprocess the \"text\" field of the batch, applying tokenization, truncation, and other relevant operations based on the specified maximum length. The function takes a batch of data, a tokenizer, and a maximum length as input. \n",
    "\n",
    "Refer to `utils/helpers.py` file for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.helpers import mlu_preprocess_batch\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "_preprocessing_function = partial(mlu_preprocess_batch, max_length=MAX_LENGTH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ba111",
   "metadata": {},
   "source": [
    "Next, apply the preprocessing function to each batch in the dataset, modifying the \"text\" field accordingly. The map operation is performed in a batched manner and the \"instruction\", \"response\", and \"text\" columns are removed from the dataset. Finally, `processed_dataset` is created by filtering `sagemaker_dataset` based on the length of the \"input_ids\" field, ensuring that the length is less than the specified `MAX_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315cacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sagemaker_dataset = sagemaker_dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"response\", \"text\"],\n",
    ")\n",
    "\n",
    "processed_dataset = encoded_sagemaker_dataset.filter(lambda rec: len(rec[\"input_ids\"]) < MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758ace3",
   "metadata": {},
   "source": [
    "Split the dataset into `train` and `test` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a705559",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb57ce",
   "metadata": {},
   "source": [
    "### <a name=\"step8\">Step 8: Define the trainer and fine-tune the LLM</a>\n",
    "\n",
    "To efficiently fine-tune a model, you will use [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685). LoRA, freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B, fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. \n",
    "\n",
    "\n",
    "#### <a name=\"#step8.1\">Step 8.1: Define the `LoraConfig` and load the LoRA model</a> \n",
    "\n",
    "You will use the build LoRA class, `LoraConfig`, from [huggingface PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft). Within `LoraConfig`, specify the following parameters:\n",
    "\n",
    "- `r`, the dimension of the low-rank matrices\n",
    "- `lora_alpha`, the scaling factor for the low-rank matrices\n",
    "- `lora_dropout`, the dropout probability of the LoRA layers\n",
    "- `task_type`, allows prompt tuning for different tasks. In our case, causal language modeling\n",
    "\n",
    "For more information about all available parameters, see Tuners on the Hugging Face PEFT page at https://huggingface.co/docs/peft/package_reference/tuners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "MICRO_BATCH_SIZE = 8  \n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LORA_R = 256 # 512\n",
    "LORA_ALPHA = 512 # 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "                 r=LORA_R,\n",
    "                 lora_alpha=LORA_ALPHA,\n",
    "                 lora_dropout=LORA_DROPOUT,\n",
    "                 bias=\"none\",\n",
    "                 task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c252d",
   "metadata": {},
   "source": [
    "Use the `get_peft_model` function to initialize the model with the LoRA framework, configuring the model based on the provided `lora_config` settings. The model can then incorporate the benefits and capabilities of the LoRA optimization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569aafa",
   "metadata": {},
   "source": [
    "As you can see above, LoRA-only trainable parameters are only about three percent of the full weights. Very efficient!\n",
    "\n",
    "#### <a name=\"#step8.2\">Step 8.2: Define the data collator</a>\n",
    "\n",
    "DataCollator is a Hugging Face transformers function that takes a list of samples from a dataset and collates them into a batch, as a dictionary of PyTorch tensors.\n",
    "\n",
    "Use `DataCollatorForCompletionOnlyLM`, which extends the functionality of the base `DataCollatorForLanguageModeling` class from the Transformers library. This custom collator is designed to handle examples where a prompt is followed by a response in the input text, and then modify the labels accordingly.\n",
    "\n",
    "Refer to `utils/helpers.py` for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import MLUDataCollatorForCompletionOnlyLM\n",
    "\n",
    "data_collator = MLUDataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c516d39",
   "metadata": {},
   "source": [
    "#### <a name=\"#step8.3\">Step 8.3: Define the trainer</a>\n",
    "\n",
    "To fine-tune the LLM, you must define a `Trainer`. First, define some training arguments.\n",
    "\n",
    "Find more information about the `Trainer`class, see Trainer on the Hugging Face Transformers page at https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60424cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4  \n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "                    fp16=True,\n",
    "                    per_device_train_batch_size=1,\n",
    "                    per_device_eval_batch_size=1,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"steps\",\n",
    "                    logging_steps=100,\n",
    "                    evaluation_strategy=\"steps\",\n",
    "                    eval_steps=100, \n",
    "                    save_strategy=\"steps\",\n",
    "                    save_steps=20000,\n",
    "                    save_total_limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90fb93",
   "metadata": {},
   "source": [
    "Now is when the magic happens! Initialize the trainer with the defined model, tokenizer, training arguments, data collator, and the train/eval datasets. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> <br/>\n",
    "The training might take about 10 minutes to run with the <code>fae/faer/faerself</code> data from <code>cda_fae_faer_faer_faerself.csv</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f634c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620a0de",
   "metadata": {},
   "source": [
    "#### <a name=\"#step8.4\">Step 8.4: Save the fine-tuned model</a>\n",
    "\n",
    "\n",
    "After the training is finished, you can save the model to a directory by using the [`transformers.PreTrainedModel.save_pretrained`] function. \n",
    "This function saves only the incremental PEFT weights (adapter_model.bin) that were trained, meaning the model is very efficient to store, transfer, and load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a41c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566df1c",
   "metadata": {},
   "source": [
    "Save the tokenizer along with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565be8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefaaad",
   "metadata": {},
   "source": [
    "### <a name=\"step9\">Step 9: Deploy the fine-tuned model</a>\n",
    "\n",
    "#### <a name=\"step9title\">Overview of deployment parameters</a>\n",
    "\n",
    "To deploy using the Amazon SageMaker Python SDK with the DJL, you must instantiate `Model` class with the following parameters:\n",
    "```{python}\n",
    "model = Model(\n",
    "    image_uri,\n",
    "    model_data=...,\n",
    "    predictor_cls=...,\n",
    "    role=aws_role\n",
    ")\n",
    "```\n",
    "- `image_uri`: The Docker image URI representing the deep learning framework and version to be used.\n",
    "- `model_data`: The location of the fine-tuned LLM model artifact in an Amazon Simple Storage Service (Amazon S3) bucket. It specifies the path to the TAR GZ file containing the model's parameters, architecture, and any necessary artifacts.\n",
    "- `predictor_cls`: This is a \"JSON in JSON out\" predictor only, nothing DJL related. For more information, see sagemaker.djl_inference.DJLPredictor at https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor.\n",
    "- `role`: The AWS Identity and Access Management (IAM) role ARN that provides necessary permissions to access resources, such as the S3 bucket containing the model data.\n",
    "\n",
    "#### <a name=\"step9.1\">Step 9.1: Instantiate SageMaker parameters</a>\n",
    "\n",
    "Initialize a SageMaker session and retrieve information related to the AWS environment, such as SageMaker role and AWS region. You also specify the image URI for a specific version of the \"djl-deepspeed\" framework using the SageMaker session's Region. The image URI is a unique identifier for a specific Docker container image that can be used in various AWS services, such as Amazon SageMaker or Amazon Elastic Container Registry (Amazon ECR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a752eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "sagemaker_session = Session()\n",
    "print(\"sagemaker_session: \", sagemaker_session)\n",
    "\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "print(\"aws_role: \", aws_role)\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "print(\"aws_region: \", aws_region)\n",
    "\n",
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\",\n",
    "                                version=\"0.22.1\",\n",
    "                                region=sagemaker_session._region_name)\n",
    "print(\"image_uri: \", image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7272de",
   "metadata": {},
   "source": [
    "#### <a name=\"step9.2\">Step 9.2: Create the model artifact</a> ###\n",
    "\n",
    "To upload the model artifact in the S3 bucket, you must create a TAR GZ file containing the model's parameters. First, create a directory named `lora_model` and a subdirectory named `dolly-3b-lora`. The \"-p\" option ensures that the command creates any intermediate directories if they don't exist. Then, copy the lora checkpoints `adapter_model.bin` and `adapter_config.json` to `dolly-3b-lora`. The base Dolly model will be downloaded at runtime from the Hugging Face hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf lora_model\n",
    "mkdir -p lora_model\n",
    "mkdir -p lora_model/dolly-3b-lora\n",
    "cp dolly-3b-lora/adapter_config.json lora_model/dolly-3b-lora/\n",
    "cp dolly-3b-lora/adapter_model.bin lora_model/dolly-3b-lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0ccb3",
   "metadata": {},
   "source": [
    "Next, set the [DJL Serving configuration options](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) in `serving.properties`. Using the Jupyter `%%writefile` magic command, you can write the following content to a file named \"lora_model/serving.properties\".\n",
    "- `engine=Python`: This line specifies the engine used for serving.\n",
    "- `option.entryPoint=model.py`: This line specifies the entry point for the serving process, which is set to \"model.py\". \n",
    "- `option.adapter_checkpoint=dolly-3b-lora`: This line sets the checkpoint for the adapter to \"dolly-3b-lora\". A checkpoint typically represents the saved state of a model or its parameters.\n",
    "- `option.adapter_name=dolly-lora`: This line sets the name of the adapter to \"dolly-lora\", a component that helps interface between the model and the serving infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96558b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora_model/serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=model.py\n",
    "option.adapter_checkpoint=dolly-3b-lora\n",
    "option.adapter_name=dolly-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae5a65",
   "metadata": {},
   "source": [
    "Another file you need in the model artifact is the environment requirement file. Create a file named `lora_model/requirements.txt`, and write a list of Python package requirements, typically used with package managers such as `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora_model/requirements.txt\n",
    "transformers==4.27.4\n",
    "accelerate>=0.24.1,<1\n",
    "peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100de81",
   "metadata": {},
   "source": [
    "#### <a name=\"step9.3\">Step 9.3: Create the inference script</a>\n",
    "\n",
    "Similar to the fine-tuning notebook, a custom pipeline `InstructionTextGenerationPipeline` is defined. The code is provided in `utils/deployment_model.py`. \n",
    "\n",
    "You save these inference functions to `lora_model/model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp utils/deployment_model.py lora_model/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce644b4d",
   "metadata": {},
   "source": [
    "#### <a name=\"step9.4\">Step 9.4: Upload the model artifact to Amazon S3</a>\n",
    "\n",
    "Create a compressed tarball archive of the \"lora_model\" directory, and then save it as \"lora_model.tar.gz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f00e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -cvzf lora_model.tar.gz lora_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144301b",
   "metadata": {},
   "source": [
    "Upload the \"lora_model.tar.gz\" file to the specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Get the name of the bucket with prefix lab-code\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name.startswith('artifact'):\n",
    "        mybucket = bucket.name\n",
    "        print(mybucket)\n",
    "    \n",
    "response = s3_client.upload_file(\"lora_model.tar.gz\", mybucket, \"lora_model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa38cd",
   "metadata": {},
   "source": [
    "#### <a name=\"step9.5\">Step 9.5: Deploy the model</a> ###\n",
    "\n",
    "Now, it's the time to deploy the fine-tuned LLM by using the SageMaker Python SDK. The SageMaker Python SDK `Model` class is instantiated with the following parameters:\n",
    "\n",
    "- `image_uri`: The Docker image URI representing the deep learning framework and version to be used.\n",
    "- `model_data`: The location of the fine-tuned LLM model artifact in an S3 bucket. It specifies the path to the TAR GZ file containing the model's parameters, architecture, and any necessary artifacts.\n",
    "- `predictor_cls`: This is a \"JSON in JSON out\" predictor only, nothing DJL related. For more information, see sagemaker.djl_inference.DJLPredictor at https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor.\n",
    "- `role`: The AWS Identity and Access Management (IAM) role ARN that provides necessary permissions to access resources, such as the S3 bucket containing the model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data=\"s3://{}/lora_model.tar.gz\".format(mybucket)\n",
    "\n",
    "model = Model(image_uri=image_uri,\n",
    "            model_data=model_data,\n",
    "            predictor_cls=sagemaker.djl_inference.DJLPredictor,\n",
    "            role=aws_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5de3b6",
   "metadata": {},
   "source": [
    "Note: The deployment should be completed within 10 minutes. If it takes longer than that, your endpoint might have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = model.deploy(1, \"ml.g4dn.2xlarge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
